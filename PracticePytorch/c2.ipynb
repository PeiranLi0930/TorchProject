{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Create *known* parameters\n",
    "weight = 0.7\n",
    "bias = 0.3\n",
    "\n",
    "# Create data\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.02\n",
    "X = torch.arange(start, end, step).unsqueeze(dim=1)\n",
    "y = weight * X + bias"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([50, 1]), torch.Size([50, 1]))"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T20:18:30.144971Z",
     "start_time": "2023-06-01T20:18:30.142381Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(40, 40, 10, 10)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create train/test split\n",
    "train_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing\n",
    "X_train, y_train = X[:train_split], y[:train_split]\n",
    "X_test, y_test = X[train_split:], y[train_split:]\n",
    "\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T20:25:00.965808Z",
     "start_time": "2023-06-01T20:25:00.961160Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def plot_predictions(train_data=X_train,\n",
    "                     train_labels=y_train,\n",
    "                     test_data=X_test,\n",
    "                     test_labels=y_test,\n",
    "                     predictions=None):\n",
    "  \"\"\"\n",
    "  Plots training data, test data and compares predictions.\n",
    "  \"\"\"\n",
    "  plt.figure(figsize=(10, 7))\n",
    "\n",
    "  # Plot training data in blue\n",
    "  plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n",
    "\n",
    "  # Plot test data in green\n",
    "  plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n",
    "\n",
    "  if predictions is not None:\n",
    "    # Plot the predictions in red (predictions were made on the test data)\n",
    "    plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n",
    "\n",
    "  # Show the legend\n",
    "  plt.legend(prop={\"size\": 14});"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T23:16:25.542666Z",
     "start_time": "2023-06-01T23:16:25.540678Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x700 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T23:16:28.835262Z",
     "start_time": "2023-06-01T23:16:28.629165Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class LinearMod(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.ly = nn.Linear(in_features = 1, out_features = 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.ly(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T23:18:58.378575Z",
     "start_time": "2023-06-01T23:18:58.374557Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Create a Linear Regression model class\n",
    "class LinearRegressionModel(\n",
    "        nn.Module):  # <- almost everything in PyTorch is a nn.Module (think of this as neural network lego blocks)\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.weights = nn.Parameter(\n",
    "      torch.randn(1,  # <- start with random weights (this will get adjusted as the model learns)\n",
    "                  dtype = torch.float),  # <- PyTorch loves float32 by default\n",
    "      requires_grad = True)  # <- can we update this value with gradient descent?)\n",
    "\n",
    "    self.bias = nn.Parameter(\n",
    "      torch.randn(1,  # <- start with random bias (this will get adjusted as the model learns)\n",
    "                  dtype = torch.float),  # <- PyTorch loves float32 by default\n",
    "      requires_grad = True)  # <- can we update this value with gradient descent?))\n",
    "\n",
    "  # Forward defines the computation in the model\n",
    "  def forward(self,\n",
    "              x: torch.Tensor) -> torch.Tensor:  # <- \"x\" is the input data (e.g. training/testing features)\n",
    "    return self.weights * x + self.bias  # <- this is the linear regression formula (y = m*x + b)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T23:23:58.997676Z",
     "start_time": "2023-06-01T23:23:58.992814Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "LinearRegressionModel()"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegressionModel()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T00:00:25.871699Z",
     "start_time": "2023-06-02T00:00:25.866864Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "[Parameter containing:\n tensor([-0.4717], requires_grad=True),\n Parameter containing:\n tensor([-1.1121], requires_grad=True)]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T00:01:23.094334Z",
     "start_time": "2023-06-02T00:01:23.088951Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('weights', tensor([-0.4717])), ('bias', tensor([-1.1121]))])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T00:02:24.780418Z",
     "start_time": "2023-06-02T00:02:24.775921Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4895],\n",
      "        [-1.4989],\n",
      "        [-1.5083],\n",
      "        [-1.5178],\n",
      "        [-1.5272],\n",
      "        [-1.5366],\n",
      "        [-1.5461],\n",
      "        [-1.5555],\n",
      "        [-1.5649],\n",
      "        [-1.5744]])\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "  y_hat = model(X_test)\n",
    "\n",
    "print(y_hat)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T00:13:26.947231Z",
     "start_time": "2023-06-02T00:13:26.940796Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x700 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(predictions = y_hat)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T00:16:45.251316Z",
     "start_time": "2023-06-02T00:16:45.113054Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(params = model.parameters(), lr = 0.01)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T00:30:42.740027Z",
     "start_time": "2023-06-02T00:30:42.737474Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 50 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 60 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 70 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 80 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 90 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 1990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 2990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 3990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 4990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 5990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 6990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 7990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 8990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 9990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 10990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 11990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 12990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 13990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 14990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 15990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 16990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 17990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 18990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 19990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 20990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 21990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 22990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 23990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 24990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 25990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 26990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 27990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 28990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 29990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 30990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 31990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 32990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 33990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 34990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 35990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 36990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 37990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 38990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 39990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 40990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 41990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 42990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 43990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 44990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 45990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 46990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 47990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 48990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49010 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49020 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49030 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49040 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49050 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49060 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49070 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49080 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49090 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49100 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49110 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49120 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49130 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49140 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49150 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49160 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49170 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49180 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49190 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49200 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49210 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49220 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49230 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49240 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49250 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49260 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49270 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49280 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49290 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49300 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49310 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49320 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49330 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49340 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49350 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49360 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49370 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49380 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49390 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49400 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49410 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49420 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49430 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49440 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49450 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49460 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49470 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49480 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49490 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49500 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49510 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49520 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49530 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49540 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49550 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49560 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49570 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49580 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49590 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49600 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49610 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49620 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49630 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49640 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49650 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49660 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49670 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49680 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49690 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49700 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49710 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49720 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49730 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49740 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49750 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49760 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49770 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49780 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49790 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49800 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49810 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49820 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49830 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49840 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49850 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49860 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49870 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49880 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49890 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49900 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49910 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49920 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49930 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49940 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49950 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49960 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49970 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49980 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 49990 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n",
      "Epoch: 50000 | MAE Train Loss: 0.00726689537987113 | MAE Test Loss: 0.008254820480942726\n"
     ]
    }
   ],
   "source": [
    "epochs = 50000\n",
    "\n",
    "train_loss_values = []\n",
    "test_loss_values = []\n",
    "epoch_count = []\n",
    "\n",
    "for i in range(1, epochs + 1):\n",
    "  model.train()\n",
    "  y_hat = model(X_train)\n",
    "  loss = loss_fn(y_hat, y_train)\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  model.eval()\n",
    "  with torch.inference_mode():\n",
    "    y_infer = model(X_test)\n",
    "    test_loss = loss_fn(y_infer, y_test.to(torch.float))\n",
    "\n",
    "    if i % 10 == 0:\n",
    "      epoch_count.append(i)\n",
    "      train_loss_values.append(loss.detach().numpy())\n",
    "      test_loss_values.append(test_loss.detach().numpy())\n",
    "      print(f\"Epoch: {i} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T01:03:08.830109Z",
     "start_time": "2023-06-02T01:03:01.891568Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_count, train_loss_values, label = \"Train Loss\")\n",
    "plt.plot(epoch_count, test_loss_values, label = \"Test Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T01:04:13.853120Z",
     "start_time": "2023-06-02T01:04:13.778279Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('weights', tensor([0.6920])), ('bias', tensor([0.2989]))])"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T01:15:13.790536Z",
     "start_time": "2023-06-02T01:15:13.780704Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x700 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(predictions = model(X_test).detach())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T01:37:05.825295Z",
     "start_time": "2023-06-02T01:37:05.678075Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to Model/model01.pth\n",
      "model01.pth saved succesfully.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ModelPath = Path(\"Model\")\n",
    "ModelPath.mkdir(parents = True, exist_ok = True)\n",
    "ModelName = \"model01.pth\"\n",
    "save_path = ModelPath / ModelName\n",
    "\n",
    "print(f\"Saving model to {save_path}\")\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"{ModelName} saved succesfully.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T01:50:04.477176Z",
     "start_time": "2023-06-02T01:50:04.465843Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "# the filepath of the data\n",
    "DATA_PATH = Path(\"data/Mnist\")\n",
    "DATA_PATH.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "URL = \"https://github.com/pytorch/tutorials/raw/main/_static/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (DATA_PATH).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (DATA_PATH).open(\"wb\").write(content)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T16:59:24.093699Z",
     "start_time": "2023-06-02T16:59:24.092085Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/Mnist/mnist.pkl.gz'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[45], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpickle\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgzip\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mgzip\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDATA_PATH\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mFILENAME\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mas_posix\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m      5\u001B[0m         ((x_train, y_train), (x_valid, y_valid), _) \u001B[38;5;241m=\u001B[39m pickle\u001B[38;5;241m.\u001B[39mload(f, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlatin-1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.8/gzip.py:58\u001B[0m, in \u001B[0;36mopen\u001B[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001B[0m\n\u001B[1;32m     56\u001B[0m gz_mode \u001B[38;5;241m=\u001B[39m mode\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mt\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(filename, (\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mbytes\u001B[39m, os\u001B[38;5;241m.\u001B[39mPathLike)):\n\u001B[0;32m---> 58\u001B[0m     binary_file \u001B[38;5;241m=\u001B[39m \u001B[43mGzipFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgz_mode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcompresslevel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(filename, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mread\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(filename, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m     60\u001B[0m     binary_file \u001B[38;5;241m=\u001B[39m GzipFile(\u001B[38;5;28;01mNone\u001B[39;00m, gz_mode, compresslevel, filename)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.8/gzip.py:173\u001B[0m, in \u001B[0;36mGzipFile.__init__\u001B[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001B[0m\n\u001B[1;32m    171\u001B[0m     mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fileobj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 173\u001B[0m     fileobj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmyfileobj \u001B[38;5;241m=\u001B[39m \u001B[43mbuiltins\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    174\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m filename \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    175\u001B[0m     filename \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(fileobj, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data/Mnist/mnist.pkl.gz'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((DATA_PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T17:32:05.796851Z",
     "start_time": "2023-06-02T17:32:05.762528Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T17:35:46.458280Z",
     "start_time": "2023-06-02T17:35:46.450427Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([-2.4076, -1.4076, -0.4076]), tensor([-2.4076, -1.4076, -0.4076]))"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "x - x.exp().sum(-1).log().unsqueeze(0), log_softmax(x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T17:40:31.433891Z",
     "start_time": "2023-06-02T17:40:31.425108Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T16:12:22.295642Z",
     "start_time": "2023-06-07T16:12:22.293039Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /Users/lipeiran/.pytorch/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/9912422 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "35a38cae41404eafb3572726796d2a2d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/lipeiran/.pytorch/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to /Users/lipeiran/.pytorch/MNIST_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /Users/lipeiran/.pytorch/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/28881 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3c1d2bb052fa44ceb455f58023ad0b0f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/lipeiran/.pytorch/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to /Users/lipeiran/.pytorch/MNIST_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /Users/lipeiran/.pytorch/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1648877 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f8ff12909a0d4a07af7b17de9c864d85"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/lipeiran/.pytorch/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to /Users/lipeiran/.pytorch/MNIST_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /Users/lipeiran/.pytorch/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4542 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c885e8df7fcb4b89bd95229596dc4b92"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/lipeiran/.pytorch/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /Users/lipeiran/.pytorch/MNIST_data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import MNIST from torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                                ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download = True, train = True, transform = transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 64, shuffle = True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.MNIST('~/.pytorch/MNIST_data/', download = True, train = False, transform = transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size = 64, shuffle = True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T16:13:43.675591Z",
     "start_time": "2023-06-07T16:13:42.037190Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-4.3034, -2.2014, -1.6661, -0.3757],\n        [-1.6502, -1.2858, -3.8486, -0.6729],\n        [-0.8805, -2.4223, -1.8391, -1.0854]])"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 4)\n",
    "\n",
    "log_softmax(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T16:30:06.749093Z",
     "start_time": "2023-06-07T16:30:06.733647Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 1])"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.exp().sum(-1).log().unsqueeze(-1).shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T16:32:38.709678Z",
     "start_time": "2023-06-07T16:32:38.705215Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
